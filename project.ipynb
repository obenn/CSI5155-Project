{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b74de16614daa0846ddfaf5c205b09a61d1fef6a850ce9d5a0bef4ebf69fbd07"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "from category_encoders import OneHotEncoder, OrdinalEncoder\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import permutations, combinations\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, plot_confusion_matrix, plot_roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_val_predict, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from pprint import pprint\n",
    "from scipy.stats import ttest_rel\n",
    "from semisupervised.StackedAutoEncoderSSL import StackedAutoEncoderClassifier\n",
    "from SAE import StackedAutoEncoder\n",
    "random_state = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('diabetic_data.csv', na_values=['?'], low_memory=False)\n",
    "df"
   ]
  },
  {
   "source": [
    "## Part A: Feature engineering, supervised learning and evaluation of results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's look at the features in the dataset referring to their descriptions in the paper, we'll decide if any transformations need to be done."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ordered_mapping = lambda x : {v: i for i, v in enumerate(x)}\n",
    "categorical_mappings = []\n",
    "onehot_columns = []"
   ]
  },
  {
   "source": [
    "### Encounter Id (encounter_id)\n",
    "\n",
    "Unique identifier of an encounter\n",
    "\n",
    "We will drop this column as it does not give us any actual qualitative information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('encounter_id', axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "### Patient number (patient_nbr)\n",
    "\n",
    "Unique identifier of a patient\n",
    "\n",
    "We will drop this column as it does not give us any actual qualitative information"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('patient_nbr', axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "### Race (race)\n",
    "\n",
    "Values: Caucasian, Asian, African American, Hispanic, and other\n",
    "\n",
    "This is a categorical variable without ordering. There appear to be 2% missing values, we will assign them to an Unknown category."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns.append('race')\n",
    "df['race'].fillna('Unknown', inplace=True)\n",
    "df['race'].value_counts(dropna=False)"
   ]
  },
  {
   "source": [
    "#### Gender (gender)\n",
    "\n",
    "Values, Male/Female, Unknown/Invalid\n",
    "\n",
    "We will encode this variable in a -1, 0, 1 fashion\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_mappings.append({\n",
    "    \"col\": \"gender\",\n",
    "    \"mapping\": {\n",
    "        \"Male\": 1,\n",
    "        \"Female\": -1,\n",
    "        \"Unknown/Invalid\": 0,\n",
    "    }\n",
    "})\n",
    "df['gender'].value_counts()"
   ]
  },
  {
   "source": [
    "### Age (age)\n",
    "\n",
    "This is an ordinal variable, we will order them accoridng to the age ranges."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_mappings.append({\n",
    "    \"col\": \"age\",\n",
    "    \"mapping\": ordered_mapping([f\"[{i}-{i+10})\" for i in range(0, 100, 10)]),\n",
    "})\n",
    "df['age'].value_counts()"
   ]
  },
  {
   "source": [
    "### Weight (weight)\n",
    "\n",
    "There look to be 98% missing values according to the paper. Let's verify and drop the column if this is the case"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['weight'].value_counts(dropna=False)"
   ]
  },
  {
   "source": [
    "Because of this disproportionality we will drop the column"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('weight', axis=1, inplace=True)"
   ]
  },
  {
   "source": [
    "### Admission type (admission_type_id)\n",
    "\n",
    "Integer identifier corresponding to 9 distinct values, for example, emergency, urgent, elective, newborn, and not available\n",
    "\n",
    "There is no obvious ordering for these so for this reason we will need to encode this as a categorical variable using one-hot encoding.\n",
    "\n",
    "We note that categories \"NULL\", \"not available\" and \"not mapped\" are all effectively unknown and we therefore combine them into one category."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns.append('admission_type_id')\n",
    "df.replace([6, 8], 5, inplace=True)\n",
    "df['admission_type_id'].value_counts(dropna=False)"
   ]
  },
  {
   "source": [
    "### Discharge disposition (discharge_disposition_id)\n",
    "\n",
    "Integer identifier corresponding to 29 distinct values, for example, discharged to home, expired, and not available.\n",
    "\n",
    "There is no obvious ordering for these so for this reason we will need to encode this as a categorical variable using one-hot encoding.\n",
    "\n",
    "We note that categories \"NULL\", \"not available\" and \"not mapped\" are all effectively unknown and we therefore combine them into one category.There does not appear to be an ordinal "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns.append('discharge_disposition_id')\n",
    "df['discharge_disposition_id'].replace([25, 26], 18, inplace=True)\n",
    "df['discharge_disposition_id'].value_counts(dropna=False)"
   ]
  },
  {
   "source": [
    "### Admission source (admission_source_id)\n",
    "\n",
    "Integer identifier corresponding to 21 distinct values, for example, physician referral, emergency room, and transfer from a hospital\n",
    "\n",
    "There is no obvious ordering for these so for this reason we will need to encode this as a categorical variable using one-hot encoding.\n",
    "\n",
    "We note that categories \"NULL\", \"not available\" and \"not mapped\" are all effectively unknown and we therefore combine them into one category.There does not appear to be an ordinal "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns.append('admission_source_id')\n",
    "df['admission_source_id'].replace([17, 21], 15)\n",
    "df['admission_source_id'].value_counts(dropna=False)"
   ]
  },
  {
   "source": [
    "### Time in hospital (time_in_hospital)\n",
    "\n",
    "Integer number of days between admission and discharge"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_in_hospital'].describe()"
   ]
  },
  {
   "source": [
    "### Payer code (payer_code)\n",
    "\n",
    "Two letter identifier corresponding to 23 distinct values, for example, Blue Cross\\Blue Shield, Medicare, and self-pay\n",
    "\n",
    "This is categorical and not ordinal so it will be one-hot encoded.\n",
    "\n",
    "Note there are 53% unknowns here"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns.append('payer_code')\n",
    "df['payer_code'].fillna('Unknown', inplace=True)\n",
    "df['payer_code'].value_counts(dropna=False)"
   ]
  },
  {
   "source": [
    "### Medical speciality (medical_specialty)\n",
    "\n",
    "Integer identifier of a specialty of the admitting physician, corresponding to 84 distinct values, for example, cardiology, internal medicine, family\\general practice, and surgeon\n",
    "\n",
    "This is categorical and not ordinal so it will be one-hot encoded.\n",
    "\n",
    "Note 53% of values are unknown here.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns.append('medical_specialty')\n",
    "df['medical_specialty'].fillna('Unknown', inplace=True)\n",
    "df['medical_specialty'].value_counts(dropna=False)"
   ]
  },
  {
   "source": [
    "### Number of lab procedures (num_lab_procedures)\n",
    "\n",
    "Number of tests performed during the encounter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_lab_procedures'].describe()"
   ]
  },
  {
   "source": [
    "### Number of procedures (num_procedures)\n",
    "\n",
    "Number of procedures (other than lab tests) performed during the encounter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_procedures'].describe()"
   ]
  },
  {
   "source": [
    "### Number of medications (num_medications)\n",
    "\n",
    "Number of distinct generic names administered during the encounter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_medications'].describe()"
   ]
  },
  {
   "source": [
    "### Number of outpatient visits (number_outpatient)\n",
    "\n",
    "Number of outpatient visits of the patient in the year preceding the encounter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_outpatient'].describe()"
   ]
  },
  {
   "source": [
    "### Number of emergency visits (number_emergency)\n",
    "\n",
    "Number of emergency visits of the patient in the year preceding the encounter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_emergency'].describe()"
   ]
  },
  {
   "source": [
    "### Number of inpatient visits (number_inpatient)\n",
    "\n",
    "Number of inpatient visits of the patient in the year preceding the encounter"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_inpatient'].describe()"
   ]
  },
  {
   "source": [
    "### Diagnoses\n",
    "\n",
    "The next three categories contain nearly a thousand distrinct values each. To reduce dimensionality we will categorize the diagnosis codes into categories according to page 5 of the paper. We note that codes 783 and 789 aren't covered and slot those into the Unknown category."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_diagnosis(diag):\n",
    "    diag = str(diag)\n",
    "    if diag == 'Unknown':\n",
    "        return 'Unknown'\n",
    "    elif diag.startswith('E') or diag.startswith('V'):\n",
    "        return 'External'\n",
    "    diag = int(float(diag))\n",
    "    if diag == 250:\n",
    "        return 'Diabetes'\n",
    "    elif (diag >= 390 and diag <= 459) or diag == 785:\n",
    "        return 'Circulatory'\n",
    "    elif (diag >= 460 and diag <= 519) or diag == 786:\n",
    "        return 'Respiratory'\n",
    "    elif (diag >= 520 and diag <= 579) or diag == 787:\n",
    "        return 'Digestive'\n",
    "    elif diag >= 800 and diag <= 999:\n",
    "        return 'Injury'\n",
    "    elif diag >= 710 and diag <= 739:\n",
    "        return 'Musculoskeletal'\n",
    "    elif (diag >= 580 and diag <= 629) or diag == 788:\n",
    "        return 'Genitourinary'\n",
    "    elif diag >= 140 and diag <= 239:\n",
    "        return 'Neoplasms'\n",
    "    elif diag == 780 or diag == 781 or diag == 784 or (diag >= 790 and diag <= 799):\n",
    "        return 'Other'\n",
    "    elif (diag >= 240 and diag <= 279) and diag != 250:\n",
    "        return 'Endocrine'\n",
    "    elif (diag >= 680 and diag <= 709) or diag == 782:\n",
    "        return 'Skin'\n",
    "    elif diag >= 1 and diag <= 139:\n",
    "        return 'Infectious'\n",
    "    elif diag >= 290 and diag <= 319:\n",
    "        return 'Mental'\n",
    "    elif diag >= 280 and diag <= 289:\n",
    "        return 'Blood'\n",
    "    elif diag >= 320 and diag <= 359:\n",
    "        return 'Nervous'\n",
    "    elif diag >= 630 and diag <= 679:\n",
    "        return 'Pregnancy'\n",
    "    elif diag >= 360 and diag <= 389:\n",
    "        return 'Sense'\n",
    "    elif diag >= 740 and diag <= 759:\n",
    "        return 'Congenital'\n",
    "    else:\n",
    "        return 'Unknown'"
   ]
  },
  {
   "source": [
    "### Diagnosis 1 (diag_1)\n",
    "\n",
    "The primary diagnosis (coded as first three digits of ICD9); 848 distinct values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns.append('diag_1')\n",
    "df['diag_1'].fillna('Unknown', inplace=True)\n",
    "df['diag_1'] = df['diag_1'].apply(categorize_diagnosis)\n",
    "df['diag_1'].value_counts()"
   ]
  },
  {
   "source": [
    "### Diagnosis 2 (diag_2)\n",
    "\n",
    "Secondary diagnosis (coded as first three digits of ICD9); 923 distinct values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns.append('diag_2')\n",
    "df['diag_2'].fillna('Unknown', inplace=True)\n",
    "df['diag_2'] = df['diag_2'].apply(categorize_diagnosis)\n",
    "df['diag_2'].value_counts()"
   ]
  },
  {
   "source": [
    "### Diagnosis 3 (diag_3)\n",
    "\n",
    "Additional secondary diagnosis (coded as first three digits of ICD9); 954 distinct values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_columns.append('diag_3')\n",
    "df['diag_3'].fillna('Unknown', inplace=True)\n",
    "df['diag_3'] = df['diag_3'].apply(categorize_diagnosis)\n",
    "df['diag_3'].value_counts()"
   ]
  },
  {
   "source": [
    "### Number of diagnoses (number_diagnoses)\n",
    "\n",
    "Number of diagnoses entered to the system"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['number_diagnoses'].describe()"
   ]
  },
  {
   "source": [
    "### Glucose serum test result (max_glu_serum)\n",
    "\n",
    "Indicates the range of the result or if the test was not taken. Values: “>200,” “>300,” “normal,” and “none” if not measured.\n",
    "\n",
    "These are ordinal, and we will make the assumption that None, Norm, >200, >300 is a sensible one."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_mappings.append({\n",
    "    'col': 'max_glu_serum',\n",
    "    'mapping': ordered_mapping(('None', 'Norm', '>200', '>300')),\n",
    "})\n",
    "df['max_glu_serum'].value_counts()"
   ]
  },
  {
   "source": [
    "### A1c test result (A1Cresult)\n",
    "\n",
    "Indicates the range of the result or if the test was not taken. Values: “>8” if the result was greater than 8%, “>7” if the result was greater than 7% but less than 8%, “normal” if the result was less than 7%, and “none” if not measured.\n",
    "\n",
    "The values are ordinal and we will assume None, Norm, >7, >8"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_mappings.append({\n",
    "    'col': 'A1Cresult',\n",
    "    'mapping': ordered_mapping(('None', 'Norm', '>7', '>8')),\n",
    "})\n",
    "df['A1Cresult'].value_counts()"
   ]
  },
  {
   "source": [
    "### Change of medications (change)\n",
    "\n",
    "Indicates if there was a change in diabetic medications (either dosage or generic name). Values: “change” and “no change”\n",
    "\n",
    "No change will be -1, and change will be 1."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_mappings.append({\n",
    "    'col': 'change',\n",
    "    'mapping': {\n",
    "        'No': -1,\n",
    "        'Ch': 1,\n",
    "    },\n",
    "})\n",
    "df['change'].value_counts()"
   ]
  },
  {
   "source": [
    "### Diabetes medications (diabetesMed)\n",
    "\n",
    "Indicates if there was any diabetic medication prescribed. Values: “yes” and “no”.\n",
    "\n",
    "Yes will be 1 and No will be -1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_mappings.append({\n",
    "    'col': 'diabetesMed',\n",
    "    'mapping': {\n",
    "        'No': -1,\n",
    "        'Yes': 1,\n",
    "    },\n",
    "})\n",
    "df['diabetesMed'].value_counts()"
   ]
  },
  {
   "source": [
    "### Readmitted (readmitted)\n",
    "\n",
    "Days to inpatient readmission. Values: “<30” if the patient was readmitted in less than 30 days, “>30” if the patient was readmitted in more than 30 days, and “No” for no record of readmission.\n",
    "\n",
    "We assume an order of NO, <30 and >30."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_mappings.append({\n",
    "    'col': 'readmitted',\n",
    "    'mapping': ordered_mapping(('NO', '<30', '>30')),\n",
    "})\n",
    "df['readmitted'].value_counts()"
   ]
  },
  {
   "source": [
    "### 24 features for medications\n",
    "\n",
    "Values: “up” if the dosage was increased during the encounter, “down” if the dosage was decreased, “steady” if the dosage did not change, and “no” if the drug was not prescribed\n",
    "\n",
    "We assume an order of No, Down, Steady and Up"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "medications = [\n",
    " 'metformin',\n",
    " 'repaglinide',\n",
    " 'nateglinide',\n",
    " 'chlorpropamide',\n",
    " 'glimepiride',\n",
    " 'acetohexamide',\n",
    " 'glipizide',\n",
    " 'glyburide',\n",
    " 'tolbutamide',\n",
    " 'pioglitazone',\n",
    " 'rosiglitazone',\n",
    " 'acarbose',\n",
    " 'miglitol',\n",
    " 'troglitazone',\n",
    " 'tolazamide',\n",
    " 'examide',\n",
    " 'citoglipton',\n",
    " 'insulin',\n",
    " 'glyburide-metformin',\n",
    " 'glipizide-metformin',\n",
    " 'glimepiride-pioglitazone',\n",
    " 'metformin-rosiglitazone',\n",
    " 'metformin-pioglitazone',\n",
    "]\n",
    "for medication in medications:\n",
    "    categorical_mappings.append({\n",
    "        'col': medication,\n",
    "        'mapping': ordered_mapping(['No', 'Down', 'Steady', 'Up']),\n",
    "    })\n",
    "    print(df[medication].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = OneHotEncoder(cols=onehot_columns, use_cat_names=True).fit_transform(OrdinalEncoder(cols=[c['col'] for c in categorical_mappings], mapping=categorical_mappings).fit_transform(df))"
   ]
  },
  {
   "source": [
    "## Task 1\n",
    "\n",
    "We will train decision tree, naive bais and knn classifiers as these support multiclass natively. We'll also investigate boosting."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "split_col = lambda d, c : (d[d.columns.difference([c])], d[c])\n",
    "\n",
    "def cv_roc(classifier, X, y, ys=None, show=True):\n",
    "    # Adapted from https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html\n",
    "    if ys is None:\n",
    "        ys = y\n",
    "    cv = KFold(n_splits=10)\n",
    "    tprs = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "        classifier.fit(X.iloc[train], ys.iloc[train])\n",
    "        viz = plot_roc_curve(classifier, X.iloc[test], y.iloc[test],\n",
    "                            name='ROC fold {}'.format(i),\n",
    "                            alpha=0.3, lw=1, ax=ax)\n",
    "        interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "        interp_tpr[0] = 0.0\n",
    "        tprs.append(interp_tpr)\n",
    "        aucs.append(viz.roc_auc)\n",
    "\n",
    "    ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',\n",
    "            label='Chance', alpha=.8)\n",
    "\n",
    "    mean_tpr = np.mean(tprs, axis=0)\n",
    "    mean_tpr[-1] = 1.0\n",
    "    mean_auc = auc(mean_fpr, mean_tpr)\n",
    "    std_auc = np.std(aucs)\n",
    "    ax.plot(mean_fpr, mean_tpr, color='b',\n",
    "            label=r'Mean ROC (AUC = %0.2f $\\pm$ %0.2f)' % (mean_auc, std_auc),\n",
    "            lw=2, alpha=.8)\n",
    "\n",
    "    std_tpr = np.std(tprs, axis=0)\n",
    "    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "    ax.fill_between(mean_fpr, tprs_lower, tprs_upper, color='grey', alpha=.2,\n",
    "                    label=r'$\\pm$ 1 std. dev.')\n",
    "\n",
    "    ax.set(xlim=[-0.05, 1.05], ylim=[-0.05, 1.05],\n",
    "        title=f\"Receiver operating characteristic for {classifier.__class__.__name__}\")\n",
    "    ax.legend(loc=\"lower right\")\n",
    "    if show:\n",
    "        plt.show()\n",
    "    return mean_auc\n",
    "\n",
    "def analyse_cm(cm, pr=True):\n",
    "    if cm is None:\n",
    "        return {\n",
    "        \"recall\": np.nan,\n",
    "        \"specificity\": np.nan,\n",
    "        \"precision\": np.nan,\n",
    "        \"F\": np.nan,\n",
    "    }\n",
    "    # We will analyse accuracy in terms of negative class (0) vs rest\n",
    "    r = range(1, len(cm))\n",
    "    tp = sum(cm[a][b] for a,b in list(permutations(r, r=2)) + list(zip(r, r)))\n",
    "    fp = sum(cm[0][i] for i in r)\n",
    "    tn = cm[0][0]\n",
    "    fn = sum(cm[i][0] for i in r)\n",
    "    recall = tpr = tp / (tp + fn)\n",
    "    specificity = tnr = tn / (tn + fp)\n",
    "    precision = ppv = tp / (tp + fp)\n",
    "    F = 2 * ((precision * recall) / (precision + recall))\n",
    "    c = {\n",
    "        \"recall\": recall,\n",
    "        \"specificity\": specificity,\n",
    "        \"precision\": precision,\n",
    "        \"F\": F,\n",
    "    }\n",
    "    if pr:\n",
    "        for k in c:\n",
    "            print(f\"{k}: {c[k]}\")\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "m_aucs = []\n",
    "scores = []\n",
    "algs = []"
   ]
  },
  {
   "source": [
    "##### Decision Tree Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier(random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "algs.append(\"DT\")\n",
    "for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "    X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "    auc_buffer += cv_roc(clf, X, y)\n",
    "m_auc = round(auc_buffer / 3, 3)\n",
    "print(f\"Mean auc score: {m_auc}\")\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "scores.append(cross_val_score(clf, X, y, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, y, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "source": [
    "##### Naive Bais"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "algs.append(\"NB\")\n",
    "for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "    X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "    auc_buffer += cv_roc(clf, X, y)\n",
    "m_auc = round(auc_buffer / 3, 3)\n",
    "print(f\"Mean auc score: {m_auc}\")\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "scores.append(cross_val_score(clf, X, y, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, y, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "source": [
    "##### K Nearest Neighbours"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's first approximate the best value for k"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(*split_col(df, \"readmitted\"), test_size=0.1, random_state=random_state)\n",
    "error = []\n",
    "\n",
    "# Calculating error for K values between 1 and 100\n",
    "min_i = None\n",
    "min_error = None\n",
    "samples = 100\n",
    "for i in range(1, samples):\n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=i, n_jobs=-1)\n",
    "    clf_knn.fit(X_train, y_train)\n",
    "    pred_i = clf_knn.predict(X_test)\n",
    "    mean_error = np.mean(pred_i != y_test)\n",
    "    if (not min_i and not min_error) or (mean_error < min_error):\n",
    "        min_i = i\n",
    "        min_error = mean_error\n",
    "    error.append(mean_error)\n",
    "print(f\"Min i is {min_i} with mean error of {min_error}\")\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(1, samples), error, color='red', linestyle='dashed', marker='o',\n",
    "         markerfacecolor='blue', markersize=10)\n",
    "plt.title('Error Rate K Value')\n",
    "plt.xlabel('K Value')\n",
    "plt.ylabel('Mean Error')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=min_i, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "algs.append(f\"kNN({min_i})\")\n",
    "for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "    X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "    auc_buffer += cv_roc(clf, X, y)\n",
    "m_auc = round(auc_buffer / 3, 3)\n",
    "print(f\"Mean auc score: {m_auc}\")\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "scores.append(cross_val_score(clf, X, y, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, y, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "source": [
    "Because of the extemely small p-value, we can confidently say these algorithms are significantly different.\n",
    "\n",
    "NB apears to perform best"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Algorithm\", \"Recall\", \"Specificity\", \"Precision\", \"F\", \"m auc\"]\n",
    "rows = []\n",
    "for cm, m_auc, alg in zip(cms, m_aucs, algs):\n",
    "    rows.append([alg, cm['recall'], cm['specificity'], cm['precision'], cm['F'], m_auc])\n",
    "pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "source": [
    "Results are pretty mediocre, let's see if we can do better with boosting."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GradientBoostingClassifier(random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "algs.append(\"BO\")\n",
    "for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "    X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "    auc_buffer += cv_roc(clf, X, y)\n",
    "m_auc = round(auc_buffer / 3, 3)\n",
    "print(f\"Mean auc score: {m_auc}\")\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "scores.append(cross_val_score(clf, X, y, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, y, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [list(range(1, 10 + 1)) + [\"mean\", 'stddev', \"p-value\"]]\n",
    "cols = [\"Fold\"]\n",
    "for a, b in combinations(range(0, len(cms)), 2):\n",
    "    sd = scores[a] - scores[b]\n",
    "    cols.append(f\"{algs[a]}-{algs[b]}\")\n",
    "    lists.append(list(sd) + [sd.mean(), sd.std(), ttest_rel(scores[a], scores[b])[1]])\n",
    "pd.DataFrame(zip(*lists), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Algorithm\", \"Recall\", \"Specificity\", \"Precision\", \"F\", \"m auc\"]\n",
    "rows = []\n",
    "for cm, m_auc, alg in zip(cms, m_aucs, algs):\n",
    "    rows.append([alg, cm['recall'], cm['specificity'], cm['precision'], cm['F'], m_auc])\n",
    "pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "source": [
    "### Task 2\n",
    "\n",
    "We're interested to see if we can predict whether a certain medication was administered. Specifically insulin.\n",
    "\n",
    "We'll use 3 models to round out the different families we have yet to explore: SVC, knn and random forest\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "m_aucs = []\n",
    "scores = []\n",
    "algs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"insulin\")\n",
    "# Turn it into a binary classifier\n",
    "y.replace([2, 3], 1, inplace=True)\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(random_state=random_state)\n",
    "algs.append(\"RF\")\n",
    "m_auc = cv_roc(clf, X, y)\n",
    "print(m_auc)\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.append(cross_val_score(clf, X, y, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, y, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=random_state)\n",
    "algs.append(\"SV\")\n",
    "m_auc = cv_roc(clf, X, y)\n",
    "print(m_auc)\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.append(cross_val_score(clf, X, y, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, y, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=20)\n",
    "algs.append(f\"kNN({min_i})\")\n",
    "m_auc = cv_roc(clf, X, y)\n",
    "print(m_auc)\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.append(cross_val_score(clf, X, y, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, y, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [list(range(1, 10 + 1)) + [\"mean\", 'stddev', \"p-value\"]]\n",
    "cols = [\"Fold\"]\n",
    "for a, b in combinations(range(0, len(cms)), 2):\n",
    "    sd = scores[a] - scores[b]\n",
    "    cols.append(f\"{algs[a]}-{algs[b]}\")\n",
    "    lists.append(list(sd) + [sd.mean(), sd.std(), ttest_rel(scores[a], scores[b])[1]])\n",
    "pd.DataFrame(zip(*lists), columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Algorithm\", \"Recall\", \"Specificity\", \"Precision\", \"F\", \"m auc\"]\n",
    "rows = []\n",
    "for cm, m_auc, alg in zip(cms, m_aucs, algs):\n",
    "    rows.append([alg, cm['recall'], cm['specificity'], cm['precision'], cm['F'], m_auc])\n",
    "pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "source": [
    "## Part B : Semi-supervised learning & evaluation of results"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms = []\n",
    "m_aucs = []\n",
    "algs = []\n",
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelPropagation(kernel='knn', n_neighbors=min_i, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "frac = 0.0\n",
    "algs.append(f\"LP({int(frac * 100)})\")\n",
    "for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "    X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "    ys = y.copy()\n",
    "    ys[ys.sample(frac=frac).index] = -1\n",
    "    auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "m_auc = round(auc_buffer / 3, 3)\n",
    "print(f\"Mean auc score: {m_auc}\")\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelPropagation(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.1\n",
    "algs.append(f\"LP({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "    X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "    ys = y.copy()\n",
    "    ys[ys.sample(frac=frac).index] = -1\n",
    "    auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "m_auc = round(auc_buffer / 3, 3)\n",
    "print(f\"Mean auc score: {m_auc}\")\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelPropagation(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.2\n",
    "algs.append(f\"LP({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "    X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "    ys = y.copy()\n",
    "    ys[ys.sample(frac=frac).index] = -1\n",
    "    auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "m_auc = round(auc_buffer / 3, 3)\n",
    "print(f\"Mean auc score: {m_auc}\")\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelPropagation(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.5\n",
    "algs.append(f\"LP({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "    X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "    ys = y.copy()\n",
    "    ys[ys.sample(frac=frac).index] = -1\n",
    "    auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "m_auc = round(auc_buffer / 3, 3)\n",
    "print(f\"Mean auc score: {m_auc}\")\n",
    "m_aucs.append(m_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelPropagation(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.9\n",
    "algs.append(f\"LP({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelPropagation(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.95\n",
    "algs.append(f\"LP({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Algorithm\", \"Recall\", \"Specificity\", \"Precision\", \"F\", \"m auc\"]\n",
    "rows = []\n",
    "for cm, m_auc, alg in zip(cms, m_aucs, algs):\n",
    "    rows.append([alg, cm['recall'], cm['specificity'], cm['precision'], cm['F'], m_auc])\n",
    "pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [list(range(1, 10 + 1)) + [\"mean\", 'stddev', \"p-value\"]]\n",
    "cols = [\"Fold\"]\n",
    "for a, b in combinations(range(0, len(cms)), 2):\n",
    "    sd = scores[a] - scores[b]\n",
    "    cols.append(f\"{algs[a]}-{algs[b]}\")\n",
    "    lists.append(list(sd) + [sd.mean(), sd.std(), ttest_rel(scores[a], scores[b])[1]])\n",
    "pd.DataFrame(zip(*lists), columns=cols)"
   ]
  },
  {
   "source": [
    "Label Spreading"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "cms = []\n",
    "algs = []\n",
    "m_aucs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelSpreading(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.0\n",
    "algs.append(f\"LS({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelSpreading(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.1\n",
    "algs.append(f\"LS({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelSpreading(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.2\n",
    "algs.append(f\"LS({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelSpreading(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.5\n",
    "algs.append(f\"LS({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelSpreading(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.9\n",
    "algs.append(f\"LS({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LabelSpreading(kernel='knn', n_neighbors=min_i, n_jobs=-1)\n",
    "frac = 0.95\n",
    "algs.append(f\"LS({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "source": [
    "cms.append(analyse_cm(cm))"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Algorithm\", \"Recall\", \"Specificity\", \"Precision\", \"F\", \"m auc\"]\n",
    "rows = []\n",
    "for cm, m_auc, alg in zip(cms, m_aucs, algs):\n",
    "    rows.append([alg, cm['recall'], cm['specificity'], cm['precision'], cm['F'], m_auc])\n",
    "pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [list(range(1, 10 + 1)) + [\"mean\", 'stddev', \"p-value\"]]\n",
    "cols = [\"Fold\"]\n",
    "for a, b in combinations(range(0, len(cms)), 2):\n",
    "    sd = scores[a] - scores[b]\n",
    "    cols.append(f\"{algs[a]}-{algs[b]}\")\n",
    "    lists.append(list(sd) + [sd.mean(), sd.std(), ttest_rel(scores[a], scores[b])[1]])\n",
    "pd.DataFrame(zip(*lists), columns=cols)"
   ]
  },
  {
   "source": [
    "Stacked Auto Encoder"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "cms = []\n",
    "algs = []\n",
    "m_aucs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_epochs=100\n",
    "finetune_epochs=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.0\n",
    "algs.append(f\"SA({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        # Regularize to 0, 1\n",
    "        y = y.apply(lambda e: e - 1 if (pair[0] == 1 or e == 2) else e)\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 2), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 3), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.1\n",
    "algs.append(f\"SA({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        # Regularize to 0, 1\n",
    "        y = y.apply(lambda e: e - 1 if (pair[0] == 1 or e == 2) else e)\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 2), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 3), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.2\n",
    "algs.append(f\"SA({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        # Regularize to 0, 1\n",
    "        y = y.apply(lambda e: e - 1 if (pair[0] == 1 or e == 2) else e)\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 2), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 3), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.5\n",
    "algs.append(f\"SA({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        # Regularize to 0, 1\n",
    "        y = y.apply(lambda e: e - 1 if (pair[0] == 1 or e == 2) else e)\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 2), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 3), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.9\n",
    "algs.append(f\"SA({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        # Regularize to 0, 1\n",
    "        y = y.apply(lambda e: e - 1 if (pair[0] == 1 or e == 2) else e)\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 2), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 3), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.95\n",
    "algs.append(f\"SA({int(frac * 100)})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_buffer = 0\n",
    "try:\n",
    "    for pair in ((0, 1), (1, 2), (0, 2)):\n",
    "        X, y = split_col(df[df[\"readmitted\"].isin(pair)], \"readmitted\")\n",
    "        # Regularize to 0, 1\n",
    "        y = y.apply(lambda e: e - 1 if (pair[0] == 1 or e == 2) else e)\n",
    "        ys = y.copy()\n",
    "        ys[ys.sample(frac=frac).index] = -1\n",
    "        clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 2), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "        auc_buffer += cv_roc(clf, X, y, ys=ys)\n",
    "    m_auc = round(auc_buffer / 3, 3)\n",
    "    print(f\"Mean auc score: {m_auc}\")\n",
    "    m_aucs.append(m_auc)\n",
    "except ValueError:\n",
    "    print(\"Error in algorithm computation\")\n",
    "    m_aucs.append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = split_col(df, \"readmitted\")\n",
    "ys = y.copy()\n",
    "ys[ys.sample(frac=frac).index] = -1\n",
    "clf = StackedAutoEncoderClassifier(StackedAutoEncoder(X.shape[1], 3), verbose=0, pretrain_epochs=pretrain_epochs, finetune_epochs=finetune_epochs)\n",
    "scores.append(cross_val_score(clf, X, ys, cv=10))\n",
    "cm = confusion_matrix(y, cross_val_predict(clf, X, ys, cv=10))\n",
    "ConfusionMatrixDisplay(cm).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cms.append(analyse_cm(cm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\"Algorithm\", \"Recall\", \"Specificity\", \"Precision\", \"F\", \"m auc\"]\n",
    "rows = []\n",
    "for cm, m_auc, alg in zip(cms, m_aucs, algs):\n",
    "    rows.append([alg, cm['recall'], cm['specificity'], cm['precision'], cm['F'], m_auc])\n",
    "pd.DataFrame(rows, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lists = [list(range(1, 10 + 1)) + [\"mean\", 'stddev', \"p-value\"]]\n",
    "cols = [\"Fold\"]\n",
    "for a, b in combinations(range(0, len(cms)), 2):\n",
    "    sd = scores[a] - scores[b]\n",
    "    cols.append(f\"{algs[a]}-{algs[b]}\")\n",
    "    lists.append(list(sd) + [sd.mean(), sd.std(), ttest_rel(scores[a], scores[b])[1]])\n",
    "pd.DataFrame(zip(*lists), columns=cols)"
   ]
  }
 ]
}